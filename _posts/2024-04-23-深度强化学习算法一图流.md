---
title: 深度强化学习算法一图流
date: 2024-04-23 09:55:55 +0800
categories: [理论]
tags: [drl]    # TAG names should always be lowercase
---

<object data="/assets/img/DRL/DRL.drawio.pdf" title="DRL.drawio.pdf" type="application/pdf"></object>

<a href="/assets/img/DRL/DRL.drawio.pdf" target="_blank">在新标签页查看</a>

## 介绍

本图展示了深度强化学习涉及到的大部分概念的关系，包括：

- 状态、动作、奖励、折扣回报
- 动作价值函数、状态价值函数、优势函数
- 最优动作价值函数、最优状态价值函数、最优优势函数

同时，本图还展示了深度强化学习中大部分算法的数学推导以及训练方法，包括：

- 价值学习
  - 离散动作空间
    - DQN
    - Dueling Network
- 策略学习
  - 离散动作空间
    - REINFORCE
    - Actor-Critic
    - REINFORCE with Baseline
    - Advantage Actor-Critic (A2C)
    - TRPO
    - PPO
  - 连续动作空间
    - DDPG
    - TD3
    - 随机高斯策略网络

## 使用方法

配合王树森[深度强化学习课程](https://github.com/wangshusen/DRL)使用。

下图对阅读顺序进行了标注：

<object data="/assets/img/DRL/DRL-numbered.drawio.pdf" title="DRL-numbered.drawio.pdf" type="application/pdf"></object>

<a href="/assets/img/DRL/DRL-numbered.drawio.pdf" target="_blank">在新标签页查看</a>

每节课程所涉及的节点标号如下所示：

1. Overview.
    - Reinforcement Learning `[1-6]`.
    - Value-Based Learning `[7-9]`.
    - Policy-Based Learning `[10-20]`.
    - Actor-Critic Methods `[21]`.
    - AlphaGo.
2. TD Learning `[22]`.
    - Sarsa.
    - Q-learning.
    - Multi-Step TD Target.
3. Advanced Topics on Value-Based Learning.
    - Experience Replay (ER) & Prioritized ER `[23]`.
    - Overestimation, Target Network, & Double DQN `[24]`.
    - Dueling Networks `[25-30]`.
4. Policy Gradient with Baseline.
    - Policy Gradient with Baseline `[31-34]`.
    - REINFORCE with Baseline `[35-37]`.
    - Advantage Actor-Critic (A2C) `[38-44]`.
    - REINFORCE versus A2C `[45]`.
5. Advanced Topics on Policy-Based Learning.
    - Trust-Region Policy Optimization (TRPO) `[46-48]`.
    - Partial Observation and RNNs.
6. Dealing with Continuous Action Space.
    - Discrete versus Continuous Control `[49]`.
    - Deterministic Policy Gradient (DPG) for Continuous Control `[50-53]`.
    - Stochastic Policy Gradient for Continuous Control `[54-60]`.
7. Multi-Agent Reinforcement Learning.
    - Basics and Challenges.
    - Centralized VS Decentralized.

> 注：图中没有标号的节点为课程中没有涉及到的内容（PPO&TD3）。如果需要进一步了解这些内容，可以参考附录中的参考资料。

## 附录

课程中没有涉及到的算法参考 [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) 文档对应算法 Notes 部分的内容 (Original Paper, OpenAI Spinning Up Guide)。

其他参考资料见图中 Reference 部分。

<style>
object {
  width: 100%;
  height: 420px;
}
</style>
